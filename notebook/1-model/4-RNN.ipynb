{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        path = tf.keras.utils.get_file('nietzsche.txt',\n",
    "            origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            self.raw_text = f.read().lower()\n",
    "        self.chars = sorted(list(set(self.raw_text)))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.text = [self.char_indices[c] for c in self.raw_text]\n",
    "\n",
    "    def get_batch(self, seq_length, batch_size):\n",
    "        seq = []\n",
    "        next_char = []\n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.text) - seq_length)\n",
    "            seq.append(self.text[index:index+seq_length])\n",
    "            next_char.append(self.text[index+seq_length])\n",
    "        return np.array(seq), np.array(next_char)       # [batch_size, seq_length], [num_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, num_chars, batch_size, seq_length):\n",
    "        super().__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.cell = tf.keras.layers.LSTMCell(units=256)\n",
    "        self.dense = tf.keras.layers.Dense(units=self.num_chars)\n",
    "\n",
    "    def call(self, inputs, from_logits=False):\n",
    "        inputs = tf.one_hot(inputs, depth=self.num_chars)       # [batch_size, seq_length, num_chars]\n",
    "        state = self.cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "        for t in range(self.seq_length):\n",
    "            output, state = self.cell(inputs[:, t, :], state)\n",
    "        logits = self.dense(output)\n",
    "        if from_logits:\n",
    "            return logits\n",
    "        else:\n",
    "            return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "seq_length = 40\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 3s 5us/step\n",
      "batch 0: loss 4.048768\n",
      "batch 1: loss 4.023171\n",
      "batch 2: loss 4.012205\n",
      "batch 3: loss 4.004153\n",
      "batch 4: loss 3.946578\n",
      "batch 5: loss 3.912091\n",
      "batch 6: loss 3.813337\n",
      "batch 7: loss 3.487267\n",
      "batch 8: loss 3.563107\n",
      "batch 9: loss 3.378630\n",
      "batch 10: loss 2.878976\n",
      "batch 11: loss 3.183325\n",
      "batch 12: loss 3.137625\n",
      "batch 13: loss 3.182850\n",
      "batch 14: loss 3.131811\n",
      "batch 15: loss 2.900273\n",
      "batch 16: loss 3.196112\n",
      "batch 17: loss 2.986158\n",
      "batch 18: loss 3.174043\n",
      "batch 19: loss 3.186116\n",
      "batch 20: loss 3.280022\n",
      "batch 21: loss 3.321677\n",
      "batch 22: loss 3.040190\n",
      "batch 23: loss 3.229928\n",
      "batch 24: loss 3.153940\n",
      "batch 25: loss 3.148842\n",
      "batch 26: loss 2.989758\n",
      "batch 27: loss 3.063774\n",
      "batch 28: loss 3.104701\n",
      "batch 29: loss 2.952697\n",
      "batch 30: loss 3.335694\n",
      "batch 31: loss 2.977010\n",
      "batch 32: loss 3.470153\n",
      "batch 33: loss 3.009651\n",
      "batch 34: loss 2.953448\n",
      "batch 35: loss 3.143293\n",
      "batch 36: loss 3.198764\n",
      "batch 37: loss 3.165862\n",
      "batch 38: loss 3.075455\n",
      "batch 39: loss 3.069132\n",
      "batch 40: loss 3.317673\n",
      "batch 41: loss 2.951719\n",
      "batch 42: loss 3.169869\n",
      "batch 43: loss 3.185408\n",
      "batch 44: loss 3.256058\n",
      "batch 45: loss 3.008366\n",
      "batch 46: loss 3.097826\n",
      "batch 47: loss 2.929995\n",
      "batch 48: loss 2.957628\n",
      "batch 49: loss 2.923766\n",
      "batch 50: loss 2.998213\n",
      "batch 51: loss 2.962221\n",
      "batch 52: loss 3.073777\n",
      "batch 53: loss 2.883975\n",
      "batch 54: loss 3.360552\n",
      "batch 55: loss 3.336214\n",
      "batch 56: loss 3.094301\n",
      "batch 57: loss 2.915845\n",
      "batch 58: loss 3.120373\n",
      "batch 59: loss 2.892552\n",
      "batch 60: loss 2.961749\n",
      "batch 61: loss 3.277893\n",
      "batch 62: loss 2.994646\n",
      "batch 63: loss 2.971042\n",
      "batch 64: loss 3.070927\n",
      "batch 65: loss 3.020834\n",
      "batch 66: loss 3.243679\n",
      "batch 67: loss 2.922452\n",
      "batch 68: loss 3.134447\n",
      "batch 69: loss 3.006645\n",
      "batch 70: loss 2.859376\n",
      "batch 71: loss 3.069566\n",
      "batch 72: loss 3.056402\n",
      "batch 73: loss 3.017364\n",
      "batch 74: loss 2.883817\n",
      "batch 75: loss 3.165750\n",
      "batch 76: loss 2.789596\n",
      "batch 77: loss 3.013501\n",
      "batch 78: loss 2.826077\n",
      "batch 79: loss 2.864620\n",
      "batch 80: loss 2.826116\n",
      "batch 81: loss 2.999349\n",
      "batch 82: loss 3.130430\n",
      "batch 83: loss 3.047916\n",
      "batch 84: loss 3.340232\n",
      "batch 85: loss 3.110523\n",
      "batch 86: loss 2.889896\n",
      "batch 87: loss 3.130695\n",
      "batch 88: loss 3.005927\n",
      "batch 89: loss 2.842305\n",
      "batch 90: loss 3.186694\n",
      "batch 91: loss 2.900428\n",
      "batch 92: loss 3.289824\n",
      "batch 93: loss 3.175136\n",
      "batch 94: loss 2.916420\n",
      "batch 95: loss 3.216902\n",
      "batch 96: loss 2.894778\n",
      "batch 97: loss 2.850994\n",
      "batch 98: loss 2.861688\n",
      "batch 99: loss 2.867653\n",
      "batch 100: loss 3.075948\n",
      "batch 101: loss 3.140792\n",
      "batch 102: loss 3.486991\n",
      "batch 103: loss 3.133899\n",
      "batch 104: loss 3.201055\n",
      "batch 105: loss 2.964328\n",
      "batch 106: loss 2.872086\n",
      "batch 107: loss 3.101043\n",
      "batch 108: loss 2.976139\n",
      "batch 109: loss 3.025774\n",
      "batch 110: loss 2.766707\n",
      "batch 111: loss 2.918656\n",
      "batch 112: loss 2.998798\n",
      "batch 113: loss 2.925309\n",
      "batch 114: loss 3.052096\n",
      "batch 115: loss 2.811414\n",
      "batch 116: loss 2.866710\n",
      "batch 117: loss 2.979845\n",
      "batch 118: loss 3.050289\n",
      "batch 119: loss 3.118812\n",
      "batch 120: loss 3.007674\n",
      "batch 121: loss 2.867205\n",
      "batch 122: loss 3.000045\n",
      "batch 123: loss 3.064001\n",
      "batch 124: loss 3.006655\n",
      "batch 125: loss 2.954988\n",
      "batch 126: loss 2.912682\n",
      "batch 127: loss 2.993821\n",
      "batch 128: loss 2.861306\n",
      "batch 129: loss 2.922973\n",
      "batch 130: loss 2.830137\n",
      "batch 131: loss 2.739419\n",
      "batch 132: loss 3.177988\n",
      "batch 133: loss 3.043234\n",
      "batch 134: loss 2.785549\n",
      "batch 135: loss 3.113784\n",
      "batch 136: loss 3.145145\n",
      "batch 137: loss 3.211121\n",
      "batch 138: loss 3.217465\n",
      "batch 139: loss 2.795963\n",
      "batch 140: loss 3.188275\n",
      "batch 141: loss 2.724406\n",
      "batch 142: loss 2.822886\n",
      "batch 143: loss 3.081667\n",
      "batch 144: loss 2.982534\n",
      "batch 145: loss 2.811404\n",
      "batch 146: loss 2.802021\n",
      "batch 147: loss 2.917318\n",
      "batch 148: loss 2.764955\n",
      "batch 149: loss 3.020114\n",
      "batch 150: loss 3.078700\n",
      "batch 151: loss 3.097767\n",
      "batch 152: loss 3.019652\n",
      "batch 153: loss 3.031123\n",
      "batch 154: loss 3.032041\n",
      "batch 155: loss 2.923528\n",
      "batch 156: loss 2.806132\n",
      "batch 157: loss 3.103935\n",
      "batch 158: loss 3.116551\n",
      "batch 159: loss 2.973732\n",
      "batch 160: loss 2.994577\n",
      "batch 161: loss 3.109783\n",
      "batch 162: loss 3.116482\n",
      "batch 163: loss 2.821194\n",
      "batch 164: loss 2.762996\n",
      "batch 165: loss 3.140340\n",
      "batch 166: loss 2.986840\n",
      "batch 167: loss 2.847646\n",
      "batch 168: loss 2.931916\n",
      "batch 169: loss 3.167196\n",
      "batch 170: loss 2.798590\n",
      "batch 171: loss 3.028482\n",
      "batch 172: loss 3.003794\n",
      "batch 173: loss 2.895809\n",
      "batch 174: loss 2.921290\n",
      "batch 175: loss 2.901550\n",
      "batch 176: loss 3.047800\n",
      "batch 177: loss 3.255751\n",
      "batch 178: loss 3.136439\n",
      "batch 179: loss 3.177379\n",
      "batch 180: loss 2.824439\n",
      "batch 181: loss 3.004930\n",
      "batch 182: loss 3.184968\n",
      "batch 183: loss 3.244337\n",
      "batch 184: loss 2.887046\n",
      "batch 185: loss 2.891339\n",
      "batch 186: loss 2.944954\n",
      "batch 187: loss 2.727303\n",
      "batch 188: loss 3.113536\n",
      "batch 189: loss 3.125725\n",
      "batch 190: loss 3.007433\n",
      "batch 191: loss 2.730177\n",
      "batch 192: loss 2.817447\n",
      "batch 193: loss 2.877372\n",
      "batch 194: loss 3.065820\n",
      "batch 195: loss 3.089522\n",
      "batch 196: loss 2.990052\n",
      "batch 197: loss 3.053276\n",
      "batch 198: loss 2.965870\n",
      "batch 199: loss 3.063300\n",
      "batch 200: loss 3.066295\n",
      "batch 201: loss 2.831486\n",
      "batch 202: loss 2.928559\n",
      "batch 203: loss 2.822579\n",
      "batch 204: loss 2.936565\n",
      "batch 205: loss 2.924614\n",
      "batch 206: loss 3.016436\n",
      "batch 207: loss 3.229118\n",
      "batch 208: loss 3.271865\n",
      "batch 209: loss 3.097708\n",
      "batch 210: loss 2.853634\n",
      "batch 211: loss 2.942523\n",
      "batch 212: loss 2.763590\n",
      "batch 213: loss 2.854573\n",
      "batch 214: loss 2.903472\n",
      "batch 215: loss 2.812838\n",
      "batch 216: loss 2.690414\n",
      "batch 217: loss 3.198896\n",
      "batch 218: loss 2.806711\n",
      "batch 219: loss 2.743563\n",
      "batch 220: loss 3.120915\n",
      "batch 221: loss 3.213628\n",
      "batch 222: loss 2.873792\n",
      "batch 223: loss 3.028980\n",
      "batch 224: loss 3.113348\n",
      "batch 225: loss 2.670143\n",
      "batch 226: loss 2.767356\n",
      "batch 227: loss 2.871233\n",
      "batch 228: loss 3.052051\n",
      "batch 229: loss 2.940433\n",
      "batch 230: loss 2.894307\n",
      "batch 231: loss 2.989499\n",
      "batch 232: loss 3.060315\n",
      "batch 233: loss 2.880072\n",
      "batch 234: loss 2.939964\n",
      "batch 235: loss 2.738703\n",
      "batch 236: loss 3.137073\n",
      "batch 237: loss 3.021360\n",
      "batch 238: loss 3.106861\n",
      "batch 239: loss 2.896429\n",
      "batch 240: loss 3.064463\n",
      "batch 241: loss 2.825567\n",
      "batch 242: loss 3.007871\n",
      "batch 243: loss 2.836241\n",
      "batch 244: loss 2.841577\n",
      "batch 245: loss 3.032724\n",
      "batch 246: loss 2.905024\n",
      "batch 247: loss 2.902055\n",
      "batch 248: loss 3.053418\n",
      "batch 249: loss 3.022993\n",
      "batch 250: loss 2.957903\n",
      "batch 251: loss 3.155950\n",
      "batch 252: loss 3.129691\n",
      "batch 253: loss 2.994761\n",
      "batch 254: loss 2.891248\n",
      "batch 255: loss 2.832557\n",
      "batch 256: loss 2.775014\n",
      "batch 257: loss 3.070474\n",
      "batch 258: loss 2.835255\n",
      "batch 259: loss 2.829892\n",
      "batch 260: loss 2.934627\n",
      "batch 261: loss 3.022087\n",
      "batch 262: loss 2.722265\n",
      "batch 263: loss 3.143043\n",
      "batch 264: loss 3.149260\n",
      "batch 265: loss 3.061668\n",
      "batch 266: loss 2.964399\n",
      "batch 267: loss 3.065694\n",
      "batch 268: loss 2.855872\n",
      "batch 269: loss 3.159810\n",
      "batch 270: loss 2.928611\n",
      "batch 271: loss 2.783132\n",
      "batch 272: loss 2.852514\n",
      "batch 273: loss 3.013309\n",
      "batch 274: loss 2.984390\n",
      "batch 275: loss 2.780929\n",
      "batch 276: loss 2.983561\n",
      "batch 277: loss 2.895333\n",
      "batch 278: loss 2.819701\n",
      "batch 279: loss 3.009740\n",
      "batch 280: loss 2.852478\n",
      "batch 281: loss 2.653864\n",
      "batch 282: loss 2.823945\n",
      "batch 283: loss 2.670806\n",
      "batch 284: loss 2.588794\n",
      "batch 285: loss 3.046396\n",
      "batch 286: loss 3.258116\n",
      "batch 287: loss 2.681393\n",
      "batch 288: loss 2.954796\n",
      "batch 289: loss 2.889920\n",
      "batch 290: loss 2.753180\n",
      "batch 291: loss 2.731724\n",
      "batch 292: loss 2.824719\n",
      "batch 293: loss 3.146650\n",
      "batch 294: loss 2.864659\n",
      "batch 295: loss 2.915927\n",
      "batch 296: loss 2.735546\n",
      "batch 297: loss 2.690182\n",
      "batch 298: loss 2.866333\n",
      "batch 299: loss 2.845311\n",
      "batch 300: loss 3.307486\n",
      "batch 301: loss 3.134024\n",
      "batch 302: loss 2.887645\n",
      "batch 303: loss 2.735234\n",
      "batch 304: loss 2.949729\n",
      "batch 305: loss 2.799447\n",
      "batch 306: loss 2.889459\n",
      "batch 307: loss 2.694602\n",
      "batch 308: loss 3.053303\n",
      "batch 309: loss 2.619238\n",
      "batch 310: loss 2.878422\n",
      "batch 311: loss 2.922229\n",
      "batch 312: loss 2.824595\n",
      "batch 313: loss 2.760235\n",
      "batch 314: loss 2.868760\n",
      "batch 315: loss 2.954713\n",
      "batch 316: loss 2.993447\n",
      "batch 317: loss 2.743386\n",
      "batch 318: loss 3.146382\n",
      "batch 319: loss 2.944199\n",
      "batch 320: loss 2.959231\n",
      "batch 321: loss 2.872299\n",
      "batch 322: loss 2.811985\n",
      "batch 323: loss 2.940120\n",
      "batch 324: loss 2.937125\n",
      "batch 325: loss 2.810009\n",
      "batch 326: loss 2.675131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 327: loss 2.980579\n",
      "batch 328: loss 2.745222\n",
      "batch 329: loss 2.703315\n",
      "batch 330: loss 2.593916\n",
      "batch 331: loss 2.871174\n",
      "batch 332: loss 2.952435\n",
      "batch 333: loss 2.853171\n",
      "batch 334: loss 2.893400\n",
      "batch 335: loss 2.775028\n",
      "batch 336: loss 2.927334\n",
      "batch 337: loss 2.988840\n",
      "batch 338: loss 2.457830\n",
      "batch 339: loss 2.850562\n",
      "batch 340: loss 3.131493\n",
      "batch 341: loss 2.867518\n",
      "batch 342: loss 2.864533\n",
      "batch 343: loss 2.772941\n",
      "batch 344: loss 2.636870\n",
      "batch 345: loss 2.775579\n",
      "batch 346: loss 2.587173\n",
      "batch 347: loss 2.700779\n",
      "batch 348: loss 2.534466\n",
      "batch 349: loss 2.798055\n",
      "batch 350: loss 2.980839\n",
      "batch 351: loss 2.883104\n",
      "batch 352: loss 2.840234\n",
      "batch 353: loss 2.933707\n",
      "batch 354: loss 2.828535\n",
      "batch 355: loss 2.918423\n",
      "batch 356: loss 2.680420\n",
      "batch 357: loss 2.728408\n",
      "batch 358: loss 2.498987\n",
      "batch 359: loss 2.778182\n",
      "batch 360: loss 2.606412\n",
      "batch 361: loss 2.527189\n",
      "batch 362: loss 2.816576\n",
      "batch 363: loss 2.863319\n",
      "batch 364: loss 2.901592\n",
      "batch 365: loss 2.632804\n",
      "batch 366: loss 2.683136\n",
      "batch 367: loss 2.994519\n",
      "batch 368: loss 2.632405\n",
      "batch 369: loss 2.578251\n",
      "batch 370: loss 2.525140\n",
      "batch 371: loss 2.548033\n",
      "batch 372: loss 2.618692\n",
      "batch 373: loss 2.863644\n",
      "batch 374: loss 2.840211\n",
      "batch 375: loss 2.548594\n",
      "batch 376: loss 2.380033\n",
      "batch 377: loss 2.817993\n",
      "batch 378: loss 2.693037\n",
      "batch 379: loss 2.535327\n",
      "batch 380: loss 2.607751\n",
      "batch 381: loss 2.704057\n",
      "batch 382: loss 2.751326\n",
      "batch 383: loss 2.850846\n",
      "batch 384: loss 2.690763\n",
      "batch 385: loss 2.591124\n",
      "batch 386: loss 2.552399\n",
      "batch 387: loss 2.593586\n",
      "batch 388: loss 2.375134\n",
      "batch 389: loss 2.644319\n",
      "batch 390: loss 2.820550\n",
      "batch 391: loss 2.867872\n",
      "batch 392: loss 2.570367\n",
      "batch 393: loss 2.763033\n",
      "batch 394: loss 2.954398\n",
      "batch 395: loss 2.776458\n",
      "batch 396: loss 2.569120\n",
      "batch 397: loss 2.473100\n",
      "batch 398: loss 2.652140\n",
      "batch 399: loss 2.907574\n",
      "batch 400: loss 2.748469\n",
      "batch 401: loss 2.186209\n",
      "batch 402: loss 2.813810\n",
      "batch 403: loss 2.485132\n",
      "batch 404: loss 2.621405\n",
      "batch 405: loss 2.742259\n",
      "batch 406: loss 2.599389\n",
      "batch 407: loss 2.387221\n",
      "batch 408: loss 2.586061\n",
      "batch 409: loss 2.706226\n",
      "batch 410: loss 2.545938\n",
      "batch 411: loss 2.610567\n",
      "batch 412: loss 2.686967\n",
      "batch 413: loss 2.877124\n",
      "batch 414: loss 2.620677\n",
      "batch 415: loss 2.749432\n",
      "batch 416: loss 2.358259\n",
      "batch 417: loss 2.640442\n",
      "batch 418: loss 2.555110\n",
      "batch 419: loss 2.504086\n",
      "batch 420: loss 2.602418\n",
      "batch 421: loss 2.566356\n",
      "batch 422: loss 2.374320\n",
      "batch 423: loss 2.471105\n",
      "batch 424: loss 2.572326\n",
      "batch 425: loss 2.829663\n",
      "batch 426: loss 2.443386\n",
      "batch 427: loss 3.069884\n",
      "batch 428: loss 2.560790\n",
      "batch 429: loss 2.846018\n",
      "batch 430: loss 2.586243\n",
      "batch 431: loss 2.681123\n",
      "batch 432: loss 2.486625\n",
      "batch 433: loss 2.801900\n",
      "batch 434: loss 2.700709\n",
      "batch 435: loss 2.434315\n",
      "batch 436: loss 2.730694\n",
      "batch 437: loss 2.774247\n",
      "batch 438: loss 2.459445\n",
      "batch 439: loss 2.637095\n",
      "batch 440: loss 2.687473\n",
      "batch 441: loss 2.822436\n",
      "batch 442: loss 2.644610\n",
      "batch 443: loss 2.608925\n",
      "batch 444: loss 2.375283\n",
      "batch 445: loss 2.683830\n",
      "batch 446: loss 2.802676\n",
      "batch 447: loss 2.440000\n",
      "batch 448: loss 2.612160\n",
      "batch 449: loss 2.374643\n",
      "batch 450: loss 2.849060\n",
      "batch 451: loss 2.565618\n",
      "batch 452: loss 2.415976\n",
      "batch 453: loss 2.650881\n",
      "batch 454: loss 2.491546\n",
      "batch 455: loss 2.868334\n",
      "batch 456: loss 2.372306\n",
      "batch 457: loss 2.807737\n",
      "batch 458: loss 2.632522\n",
      "batch 459: loss 2.544788\n",
      "batch 460: loss 2.804964\n",
      "batch 461: loss 2.406552\n",
      "batch 462: loss 2.433065\n",
      "batch 463: loss 2.426902\n",
      "batch 464: loss 2.626763\n",
      "batch 465: loss 2.559659\n",
      "batch 466: loss 2.454073\n",
      "batch 467: loss 2.945027\n",
      "batch 468: loss 2.616371\n",
      "batch 469: loss 2.533995\n",
      "batch 470: loss 2.585976\n",
      "batch 471: loss 2.189147\n",
      "batch 472: loss 2.562526\n",
      "batch 473: loss 2.291614\n",
      "batch 474: loss 2.364357\n",
      "batch 475: loss 2.789988\n",
      "batch 476: loss 2.527810\n",
      "batch 477: loss 2.554681\n",
      "batch 478: loss 2.892764\n",
      "batch 479: loss 2.624692\n",
      "batch 480: loss 2.963894\n",
      "batch 481: loss 2.640479\n",
      "batch 482: loss 2.404969\n",
      "batch 483: loss 2.784629\n",
      "batch 484: loss 2.444262\n",
      "batch 485: loss 2.509228\n",
      "batch 486: loss 2.553812\n",
      "batch 487: loss 2.607960\n",
      "batch 488: loss 2.624107\n",
      "batch 489: loss 2.679469\n",
      "batch 490: loss 2.517716\n",
      "batch 491: loss 2.770530\n",
      "batch 492: loss 2.772642\n",
      "batch 493: loss 2.474729\n",
      "batch 494: loss 2.399838\n",
      "batch 495: loss 2.670214\n",
      "batch 496: loss 2.385558\n",
      "batch 497: loss 2.930254\n",
      "batch 498: loss 2.718449\n",
      "batch 499: loss 2.363416\n",
      "batch 500: loss 2.586332\n",
      "batch 501: loss 2.614178\n",
      "batch 502: loss 2.623154\n",
      "batch 503: loss 2.708455\n",
      "batch 504: loss 2.641804\n",
      "batch 505: loss 2.417331\n",
      "batch 506: loss 2.545980\n",
      "batch 507: loss 3.047658\n",
      "batch 508: loss 2.779955\n",
      "batch 509: loss 2.388486\n",
      "batch 510: loss 2.585338\n",
      "batch 511: loss 2.557302\n",
      "batch 512: loss 2.439734\n",
      "batch 513: loss 2.696798\n",
      "batch 514: loss 2.506802\n",
      "batch 515: loss 2.749101\n",
      "batch 516: loss 2.612095\n",
      "batch 517: loss 2.587132\n",
      "batch 518: loss 2.391747\n",
      "batch 519: loss 2.512607\n",
      "batch 520: loss 2.612966\n",
      "batch 521: loss 2.318114\n",
      "batch 522: loss 2.719183\n",
      "batch 523: loss 2.516163\n",
      "batch 524: loss 2.924865\n",
      "batch 525: loss 2.706731\n",
      "batch 526: loss 2.475283\n",
      "batch 527: loss 2.450445\n",
      "batch 528: loss 2.508492\n",
      "batch 529: loss 2.765534\n",
      "batch 530: loss 2.667329\n",
      "batch 531: loss 2.975876\n",
      "batch 532: loss 2.730047\n",
      "batch 533: loss 2.468265\n",
      "batch 534: loss 2.419892\n",
      "batch 535: loss 2.732858\n",
      "batch 536: loss 2.674576\n",
      "batch 537: loss 2.615655\n",
      "batch 538: loss 2.772030\n",
      "batch 539: loss 2.267669\n",
      "batch 540: loss 2.803426\n",
      "batch 541: loss 2.429477\n",
      "batch 542: loss 2.307473\n",
      "batch 543: loss 2.546267\n",
      "batch 544: loss 2.847645\n",
      "batch 545: loss 2.515300\n",
      "batch 546: loss 2.441417\n",
      "batch 547: loss 2.491576\n",
      "batch 548: loss 2.477521\n",
      "batch 549: loss 2.388721\n",
      "batch 550: loss 2.555865\n",
      "batch 551: loss 2.534313\n",
      "batch 552: loss 2.532179\n",
      "batch 553: loss 2.507579\n",
      "batch 554: loss 2.640711\n",
      "batch 555: loss 2.334584\n",
      "batch 556: loss 2.294616\n",
      "batch 557: loss 2.398198\n",
      "batch 558: loss 2.707250\n",
      "batch 559: loss 2.574100\n",
      "batch 560: loss 2.515521\n",
      "batch 561: loss 2.571734\n",
      "batch 562: loss 2.798459\n",
      "batch 563: loss 2.560878\n",
      "batch 564: loss 2.525807\n",
      "batch 565: loss 2.543618\n",
      "batch 566: loss 2.680607\n",
      "batch 567: loss 2.497779\n",
      "batch 568: loss 2.630464\n",
      "batch 569: loss 2.496242\n",
      "batch 570: loss 2.800046\n",
      "batch 571: loss 2.479484\n",
      "batch 572: loss 2.352485\n",
      "batch 573: loss 2.739746\n",
      "batch 574: loss 2.452397\n",
      "batch 575: loss 2.546931\n",
      "batch 576: loss 2.662219\n",
      "batch 577: loss 2.643719\n",
      "batch 578: loss 2.319151\n",
      "batch 579: loss 2.698337\n",
      "batch 580: loss 2.554054\n",
      "batch 581: loss 2.357946\n",
      "batch 582: loss 2.885769\n",
      "batch 583: loss 2.734877\n",
      "batch 584: loss 2.603365\n",
      "batch 585: loss 2.635489\n",
      "batch 586: loss 2.490645\n",
      "batch 587: loss 2.583305\n",
      "batch 588: loss 2.486519\n",
      "batch 589: loss 2.841676\n",
      "batch 590: loss 2.487260\n",
      "batch 591: loss 2.502348\n",
      "batch 592: loss 2.695394\n",
      "batch 593: loss 2.511428\n",
      "batch 594: loss 2.597142\n",
      "batch 595: loss 2.506846\n",
      "batch 596: loss 2.639783\n",
      "batch 597: loss 2.542103\n",
      "batch 598: loss 2.458456\n",
      "batch 599: loss 2.487683\n",
      "batch 600: loss 2.821082\n",
      "batch 601: loss 2.775583\n",
      "batch 602: loss 2.531582\n",
      "batch 603: loss 2.879488\n",
      "batch 604: loss 2.385908\n",
      "batch 605: loss 2.606214\n",
      "batch 606: loss 2.761893\n",
      "batch 607: loss 2.724827\n",
      "batch 608: loss 2.580329\n",
      "batch 609: loss 2.389964\n",
      "batch 610: loss 2.327721\n",
      "batch 611: loss 2.385529\n",
      "batch 612: loss 2.337846\n",
      "batch 613: loss 2.267140\n",
      "batch 614: loss 2.685736\n",
      "batch 615: loss 2.498710\n",
      "batch 616: loss 2.869404\n",
      "batch 617: loss 2.524278\n",
      "batch 618: loss 2.449326\n",
      "batch 619: loss 2.626286\n",
      "batch 620: loss 1.928988\n",
      "batch 621: loss 2.341257\n",
      "batch 622: loss 2.441109\n",
      "batch 623: loss 2.389901\n",
      "batch 624: loss 2.320578\n",
      "batch 625: loss 2.580786\n",
      "batch 626: loss 2.481150\n",
      "batch 627: loss 2.750656\n",
      "batch 628: loss 2.538734\n",
      "batch 629: loss 2.419678\n",
      "batch 630: loss 2.206691\n",
      "batch 631: loss 2.666978\n",
      "batch 632: loss 2.138277\n",
      "batch 633: loss 2.430294\n",
      "batch 634: loss 2.408663\n",
      "batch 635: loss 2.848497\n",
      "batch 636: loss 2.718452\n",
      "batch 637: loss 2.542248\n",
      "batch 638: loss 2.443887\n",
      "batch 639: loss 2.475193\n",
      "batch 640: loss 2.069755\n",
      "batch 641: loss 2.623694\n",
      "batch 642: loss 2.669557\n",
      "batch 643: loss 2.242353\n",
      "batch 644: loss 2.500078\n",
      "batch 645: loss 2.518426\n",
      "batch 646: loss 2.422299\n",
      "batch 647: loss 2.521970\n",
      "batch 648: loss 2.209462\n",
      "batch 649: loss 2.446142\n",
      "batch 650: loss 2.680550\n",
      "batch 651: loss 2.813829\n",
      "batch 652: loss 2.609779\n",
      "batch 653: loss 2.329604\n",
      "batch 654: loss 2.484511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 655: loss 2.467494\n",
      "batch 656: loss 2.463978\n",
      "batch 657: loss 2.566087\n",
      "batch 658: loss 2.308928\n",
      "batch 659: loss 2.590937\n",
      "batch 660: loss 2.520720\n",
      "batch 661: loss 2.198870\n",
      "batch 662: loss 2.411905\n",
      "batch 663: loss 2.260876\n",
      "batch 664: loss 2.417760\n",
      "batch 665: loss 2.405300\n",
      "batch 666: loss 2.644347\n",
      "batch 667: loss 2.443368\n",
      "batch 668: loss 2.646087\n",
      "batch 669: loss 2.607146\n",
      "batch 670: loss 2.387889\n",
      "batch 671: loss 2.239663\n",
      "batch 672: loss 2.235370\n",
      "batch 673: loss 2.661579\n",
      "batch 674: loss 2.568810\n",
      "batch 675: loss 2.325109\n",
      "batch 676: loss 2.505965\n",
      "batch 677: loss 2.190019\n",
      "batch 678: loss 2.594875\n",
      "batch 679: loss 2.250072\n",
      "batch 680: loss 2.224346\n",
      "batch 681: loss 2.310043\n",
      "batch 682: loss 2.597472\n",
      "batch 683: loss 2.689369\n",
      "batch 684: loss 2.672514\n",
      "batch 685: loss 2.470024\n",
      "batch 686: loss 2.283690\n",
      "batch 687: loss 2.676895\n",
      "batch 688: loss 2.535475\n",
      "batch 689: loss 2.466646\n",
      "batch 690: loss 2.746387\n",
      "batch 691: loss 2.629855\n",
      "batch 692: loss 2.671843\n",
      "batch 693: loss 2.546695\n",
      "batch 694: loss 2.803538\n",
      "batch 695: loss 2.416388\n",
      "batch 696: loss 2.389398\n",
      "batch 697: loss 2.280726\n",
      "batch 698: loss 2.634550\n",
      "batch 699: loss 2.402884\n",
      "batch 700: loss 2.795458\n",
      "batch 701: loss 2.747935\n",
      "batch 702: loss 2.161525\n",
      "batch 703: loss 2.601725\n",
      "batch 704: loss 2.302223\n",
      "batch 705: loss 2.649305\n",
      "batch 706: loss 2.938504\n",
      "batch 707: loss 2.834841\n",
      "batch 708: loss 2.319752\n",
      "batch 709: loss 2.462528\n",
      "batch 710: loss 2.573629\n",
      "batch 711: loss 2.401623\n",
      "batch 712: loss 2.630662\n",
      "batch 713: loss 2.509393\n",
      "batch 714: loss 2.604094\n",
      "batch 715: loss 2.680137\n",
      "batch 716: loss 2.707226\n",
      "batch 717: loss 2.789627\n",
      "batch 718: loss 2.614856\n",
      "batch 719: loss 2.618799\n",
      "batch 720: loss 2.737719\n",
      "batch 721: loss 2.507886\n",
      "batch 722: loss 2.134857\n",
      "batch 723: loss 2.569219\n",
      "batch 724: loss 2.391468\n",
      "batch 725: loss 2.361418\n",
      "batch 726: loss 2.552319\n",
      "batch 727: loss 2.502539\n",
      "batch 728: loss 2.618360\n",
      "batch 729: loss 2.154126\n",
      "batch 730: loss 2.551045\n",
      "batch 731: loss 2.514439\n",
      "batch 732: loss 2.949193\n",
      "batch 733: loss 2.211047\n",
      "batch 734: loss 2.705289\n",
      "batch 735: loss 2.447385\n",
      "batch 736: loss 2.533980\n",
      "batch 737: loss 2.558021\n",
      "batch 738: loss 2.350913\n",
      "batch 739: loss 2.403948\n",
      "batch 740: loss 2.820667\n",
      "batch 741: loss 2.480301\n",
      "batch 742: loss 2.482726\n",
      "batch 743: loss 2.203954\n",
      "batch 744: loss 2.661481\n",
      "batch 745: loss 2.304248\n",
      "batch 746: loss 2.654780\n",
      "batch 747: loss 2.676343\n",
      "batch 748: loss 2.360379\n",
      "batch 749: loss 2.213350\n",
      "batch 750: loss 2.391586\n",
      "batch 751: loss 2.336973\n",
      "batch 752: loss 2.398839\n",
      "batch 753: loss 2.464188\n",
      "batch 754: loss 2.519369\n",
      "batch 755: loss 2.390617\n",
      "batch 756: loss 2.345502\n",
      "batch 757: loss 2.753208\n",
      "batch 758: loss 2.443996\n",
      "batch 759: loss 2.313749\n",
      "batch 760: loss 2.518846\n",
      "batch 761: loss 2.497512\n",
      "batch 762: loss 2.289462\n",
      "batch 763: loss 2.494020\n",
      "batch 764: loss 2.311545\n",
      "batch 765: loss 2.348103\n",
      "batch 766: loss 2.510675\n",
      "batch 767: loss 2.571249\n",
      "batch 768: loss 2.588200\n",
      "batch 769: loss 2.645056\n",
      "batch 770: loss 2.339886\n",
      "batch 771: loss 2.441119\n",
      "batch 772: loss 2.600226\n",
      "batch 773: loss 2.339158\n",
      "batch 774: loss 2.611722\n",
      "batch 775: loss 2.818940\n",
      "batch 776: loss 2.461232\n",
      "batch 777: loss 2.423551\n",
      "batch 778: loss 2.261690\n",
      "batch 779: loss 2.493656\n",
      "batch 780: loss 2.358911\n",
      "batch 781: loss 2.479476\n",
      "batch 782: loss 2.508773\n",
      "batch 783: loss 2.303348\n",
      "batch 784: loss 2.277013\n",
      "batch 785: loss 2.158413\n",
      "batch 786: loss 2.509313\n",
      "batch 787: loss 2.681782\n",
      "batch 788: loss 2.297584\n",
      "batch 789: loss 2.517392\n",
      "batch 790: loss 2.316745\n",
      "batch 791: loss 2.519801\n",
      "batch 792: loss 2.329876\n",
      "batch 793: loss 2.556880\n",
      "batch 794: loss 2.245898\n",
      "batch 795: loss 2.238879\n",
      "batch 796: loss 2.503053\n",
      "batch 797: loss 2.606488\n",
      "batch 798: loss 2.249683\n",
      "batch 799: loss 2.274451\n",
      "batch 800: loss 2.727754\n",
      "batch 801: loss 2.356568\n",
      "batch 802: loss 2.373373\n",
      "batch 803: loss 2.494107\n",
      "batch 804: loss 2.427826\n",
      "batch 805: loss 2.572093\n",
      "batch 806: loss 2.555807\n",
      "batch 807: loss 2.331807\n",
      "batch 808: loss 2.635648\n",
      "batch 809: loss 2.180999\n",
      "batch 810: loss 2.682903\n",
      "batch 811: loss 2.446395\n",
      "batch 812: loss 2.593021\n",
      "batch 813: loss 2.613677\n",
      "batch 814: loss 2.575311\n",
      "batch 815: loss 2.232414\n",
      "batch 816: loss 2.437643\n",
      "batch 817: loss 2.571558\n",
      "batch 818: loss 2.508380\n",
      "batch 819: loss 2.497711\n",
      "batch 820: loss 2.817396\n",
      "batch 821: loss 2.478904\n",
      "batch 822: loss 2.496734\n",
      "batch 823: loss 2.266850\n",
      "batch 824: loss 2.369150\n",
      "batch 825: loss 2.496453\n",
      "batch 826: loss 2.479967\n",
      "batch 827: loss 2.344841\n",
      "batch 828: loss 2.078288\n",
      "batch 829: loss 2.509686\n",
      "batch 830: loss 2.109705\n",
      "batch 831: loss 2.547102\n",
      "batch 832: loss 2.361096\n",
      "batch 833: loss 2.506028\n",
      "batch 834: loss 2.444371\n",
      "batch 835: loss 2.716913\n",
      "batch 836: loss 2.273108\n",
      "batch 837: loss 2.536793\n",
      "batch 838: loss 2.471943\n",
      "batch 839: loss 2.522774\n",
      "batch 840: loss 2.703076\n",
      "batch 841: loss 2.472998\n",
      "batch 842: loss 2.316121\n",
      "batch 843: loss 2.346896\n",
      "batch 844: loss 2.632655\n",
      "batch 845: loss 2.272462\n",
      "batch 846: loss 2.490566\n",
      "batch 847: loss 2.362352\n",
      "batch 848: loss 2.269457\n",
      "batch 849: loss 2.274049\n",
      "batch 850: loss 2.065479\n",
      "batch 851: loss 2.583723\n",
      "batch 852: loss 2.191794\n",
      "batch 853: loss 2.827390\n",
      "batch 854: loss 2.366354\n",
      "batch 855: loss 2.412151\n",
      "batch 856: loss 2.826826\n",
      "batch 857: loss 2.348964\n",
      "batch 858: loss 2.195007\n",
      "batch 859: loss 2.432725\n",
      "batch 860: loss 2.456816\n",
      "batch 861: loss 2.518398\n",
      "batch 862: loss 2.430489\n",
      "batch 863: loss 2.316384\n",
      "batch 864: loss 2.890015\n",
      "batch 865: loss 2.388309\n",
      "batch 866: loss 2.276622\n",
      "batch 867: loss 2.373894\n",
      "batch 868: loss 2.508718\n",
      "batch 869: loss 2.238204\n",
      "batch 870: loss 2.517042\n",
      "batch 871: loss 2.843769\n",
      "batch 872: loss 2.438895\n",
      "batch 873: loss 2.655272\n",
      "batch 874: loss 2.403739\n",
      "batch 875: loss 2.276190\n",
      "batch 876: loss 2.139350\n",
      "batch 877: loss 2.511577\n",
      "batch 878: loss 2.504210\n",
      "batch 879: loss 2.413409\n",
      "batch 880: loss 2.682085\n",
      "batch 881: loss 2.350429\n",
      "batch 882: loss 2.403824\n",
      "batch 883: loss 2.239990\n",
      "batch 884: loss 2.529755\n",
      "batch 885: loss 2.113685\n",
      "batch 886: loss 2.273494\n",
      "batch 887: loss 2.583416\n",
      "batch 888: loss 2.571591\n",
      "batch 889: loss 2.290220\n",
      "batch 890: loss 2.427979\n",
      "batch 891: loss 2.422256\n",
      "batch 892: loss 2.479264\n",
      "batch 893: loss 2.247940\n",
      "batch 894: loss 2.353467\n",
      "batch 895: loss 2.632258\n",
      "batch 896: loss 2.315442\n",
      "batch 897: loss 2.286828\n",
      "batch 898: loss 2.317524\n",
      "batch 899: loss 2.239226\n",
      "batch 900: loss 2.586429\n",
      "batch 901: loss 2.683465\n",
      "batch 902: loss 2.490309\n",
      "batch 903: loss 2.471771\n",
      "batch 904: loss 2.770320\n",
      "batch 905: loss 2.575664\n",
      "batch 906: loss 2.329311\n",
      "batch 907: loss 2.401150\n",
      "batch 908: loss 2.168525\n",
      "batch 909: loss 2.597152\n",
      "batch 910: loss 2.507855\n",
      "batch 911: loss 2.329666\n",
      "batch 912: loss 2.491892\n",
      "batch 913: loss 2.098879\n",
      "batch 914: loss 2.204315\n",
      "batch 915: loss 2.627624\n",
      "batch 916: loss 2.362918\n",
      "batch 917: loss 2.897789\n",
      "batch 918: loss 2.284401\n",
      "batch 919: loss 2.369957\n",
      "batch 920: loss 2.398762\n",
      "batch 921: loss 2.012751\n",
      "batch 922: loss 2.558741\n",
      "batch 923: loss 2.600392\n",
      "batch 924: loss 2.449138\n",
      "batch 925: loss 2.233744\n",
      "batch 926: loss 2.777869\n",
      "batch 927: loss 1.999210\n",
      "batch 928: loss 2.499450\n",
      "batch 929: loss 2.051315\n",
      "batch 930: loss 2.339287\n",
      "batch 931: loss 2.385632\n",
      "batch 932: loss 2.346646\n",
      "batch 933: loss 2.625552\n",
      "batch 934: loss 2.179378\n",
      "batch 935: loss 2.139294\n",
      "batch 936: loss 2.412659\n",
      "batch 937: loss 2.334051\n",
      "batch 938: loss 2.371704\n",
      "batch 939: loss 2.640519\n",
      "batch 940: loss 2.154971\n",
      "batch 941: loss 2.263041\n",
      "batch 942: loss 2.677526\n",
      "batch 943: loss 2.188412\n",
      "batch 944: loss 2.530849\n",
      "batch 945: loss 2.548243\n",
      "batch 946: loss 2.400388\n",
      "batch 947: loss 2.480838\n",
      "batch 948: loss 2.313072\n",
      "batch 949: loss 2.287841\n",
      "batch 950: loss 2.408066\n",
      "batch 951: loss 2.344500\n",
      "batch 952: loss 2.717633\n",
      "batch 953: loss 2.447093\n",
      "batch 954: loss 2.234373\n",
      "batch 955: loss 2.678390\n",
      "batch 956: loss 2.222989\n",
      "batch 957: loss 2.501207\n",
      "batch 958: loss 2.373900\n",
      "batch 959: loss 2.708642\n",
      "batch 960: loss 2.701199\n",
      "batch 961: loss 2.308105\n",
      "batch 962: loss 2.560857\n",
      "batch 963: loss 2.164630\n",
      "batch 964: loss 1.999955\n",
      "batch 965: loss 2.335953\n",
      "batch 966: loss 2.596904\n",
      "batch 967: loss 2.364000\n",
      "batch 968: loss 2.283547\n",
      "batch 969: loss 2.217898\n",
      "batch 970: loss 2.268929\n",
      "batch 971: loss 2.480623\n",
      "batch 972: loss 2.443969\n",
      "batch 973: loss 2.292668\n",
      "batch 974: loss 2.064038\n",
      "batch 975: loss 2.499571\n",
      "batch 976: loss 2.360442\n",
      "batch 977: loss 2.177223\n",
      "batch 978: loss 2.365403\n",
      "batch 979: loss 2.316018\n",
      "batch 980: loss 2.215307\n",
      "batch 981: loss 2.117335\n",
      "batch 982: loss 2.191210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 983: loss 2.503717\n",
      "batch 984: loss 2.011304\n",
      "batch 985: loss 2.443228\n",
      "batch 986: loss 2.431129\n",
      "batch 987: loss 2.397177\n",
      "batch 988: loss 2.500897\n",
      "batch 989: loss 2.345928\n",
      "batch 990: loss 2.151245\n",
      "batch 991: loss 2.568460\n",
      "batch 992: loss 2.440712\n",
      "batch 993: loss 2.529739\n",
      "batch 994: loss 2.174102\n",
      "batch 995: loss 2.903484\n",
      "batch 996: loss 2.484757\n",
      "batch 997: loss 2.448186\n",
      "batch 998: loss 2.638718\n",
      "batch 999: loss 2.212423\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader()\n",
    "model = RNN(num_chars=len(data_loader.chars), batch_size=batch_size, seq_length=seq_length)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(seq_length, batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, inputs, temperature=1.):\n",
    "    batch_size, _ = tf.shape(inputs)\n",
    "    logits = self(inputs, from_logits=True)\n",
    "    prob = tf.nn.softmax(logits / temperature).numpy()\n",
    "    return np.array([np.random.choice(self.num_chars, p=prob[i, :])\n",
    "                     for i in range(batch_size.numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity 0.200000:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-952409d6c20c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"diversity %f:\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.07/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples_or_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_end\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbatch_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;31m# Slice into a batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "X_, _ = data_loader.get_batch(seq_length, 1)\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    X = X_\n",
    "    print(\"diversity %f:\" % diversity)\n",
    "    for t in range(400):\n",
    "        y_pred = model.predict(X, diversity)\n",
    "        print(data_loader.indices_char[y_pred[0]], end='', flush=True)\n",
    "        X = np.concatenate([X[:, 1:], np.expand_dims(y_pred, axis=1)], axis=-1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
